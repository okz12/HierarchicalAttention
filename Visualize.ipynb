{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T05:59:43.853242Z",
     "start_time": "2021-01-18T05:59:43.848149Z"
    }
   },
   "outputs": [],
   "source": [
    "document = {\n",
    "    'classification': 'Finance',\n",
    "    'score': 0.7,\n",
    "    'lines': [\n",
    "        {\n",
    "            'score': 0.85,\n",
    "            'tokens': [\n",
    "                {'text': 'This', 'score': 0.20},\n",
    "                {'text': 'is', 'score': 0.30},\n",
    "                {'text': 'a', 'score': 0.40},\n",
    "                {'text': 'test', 'score': 0.60},\n",
    "                {'text': 'example', 'score': 0.70}\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            'score': 0.25,\n",
    "            'tokens': [\n",
    "                {'text': 'Can', 'score': 0.90},\n",
    "                {'text': 'it', 'score': 0.60},\n",
    "                {'text': 'display', 'score': 0.30},\n",
    "                {'text': 'well?', 'score': 0.20}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T06:02:43.227540Z",
     "start_time": "2021-01-18T06:02:43.221555Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Javascript, display, HTML\n",
    "import json\n",
    "\n",
    "with open('vis.js', 'r') as f:\n",
    "    vis = f.read()\n",
    "    \n",
    "def output_doc(doc) -> None:\n",
    "    display(Javascript(vis + f\"outputHAN(element, {json.dumps(doc)});\"))\n",
    "    \n",
    "output_doc(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T05:19:02.966468Z",
     "start_time": "2021-01-18T05:18:19.987853Z"
    }
   },
   "outputs": [],
   "source": [
    "from utils import HANDataset\n",
    "import pytorch_lightning as pl\n",
    "from model import HierarchicalAttentionNetwork, Preprocessor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "pretrained_embedding_model = 'distilroberta-base'\n",
    "embedding_layer = AutoModel.from_pretrained(pretrained_embedding_model).get_input_embeddings()\n",
    "pre = Preprocessor(PunktSentenceTokenizer(), AutoTokenizer.from_pretrained(pretrained_embedding_model, use_fast=True))\n",
    "\n",
    "model = HierarchicalAttentionNetwork(n_classes = 10, \n",
    "                                    embedding_layer = embedding_layer,\n",
    "                                    embedding_size = 768,\n",
    "                                    fine_tune_embeddings = False, \n",
    "                                    word_rnn_size = 50, \n",
    "                                    sentence_rnn_size = 50, \n",
    "                                    word_rnn_layers = 1,\n",
    "                                    sentence_rnn_layers = 1, \n",
    "                                    word_att_size = 100, # size of the word-level attention layer (also the size of the word context vector)\n",
    "                                    sentence_att_size = 100, # size of the sentence-level attention layer (also the size of the sentence context vector)\n",
    "                                    dropout = 0.3)\n",
    "model.load_state_dict(torch.load('model.pth', map_location = 'cpu'), strict = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T05:19:03.712226Z",
     "start_time": "2021-01-18T05:19:02.984998Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "with open ('data/yahoo_answers_csv/classes.txt') as f:\n",
    "    classes = f.read()\n",
    "rev_label_map = {i: c for i, c in enumerate(classes.split(\"\\n\")[:-1])}\n",
    "\n",
    "def dataframe_process(df):\n",
    "    df = df.fillna('')\n",
    "    df['Text'] = 'Q. ' + df['Question'] + ' ' + df['Question Desc'] + ' A. ' + df['Answers']\n",
    "    df = df.drop(['Question', 'Question Desc', 'Answers'], axis=1)\n",
    "    df['Label'] = df['Label'] - 1\n",
    "    return df\n",
    "test = pd.read_csv(\"./data/yahoo_answers_csv/test.csv\", header = None, names = ['Label', 'Question', 'Question Desc','Answers'])\n",
    "test = dataframe_process(test)\n",
    "X_test, y_test = list(test['Text']), list(test['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T05:19:06.948572Z",
     "start_time": "2021-01-18T05:19:03.714999Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "pretrained_embedding_model = 'distilroberta-base'\n",
    "embedding_layer = AutoModel.from_pretrained(pretrained_embedding_model).get_input_embeddings()\n",
    "pre = Preprocessor(PunktSentenceTokenizer(), AutoTokenizer.from_pretrained(pretrained_embedding_model, use_fast=True))\n",
    "\n",
    "from typing import Dict, Any\n",
    "def inference(model: model.HierarchicalAttentionNetwork,\n",
    "              pre: model.preprocessor,\n",
    "              rev_label_map: Dict[int, str],\n",
    "              sample: str) -> Dict[Any]:\n",
    "    doc, sentences_in_doc, words_in_each_sentence = pre.encode_document(sample, max_words=70, max_sentences=len(pre.sentence_tokenizer.tokenize(sample)))\n",
    "    doc = torch.LongTensor(doc).unsqueeze(0)\n",
    "    sentences_in_doc = torch.LongTensor([sentences_in_doc])\n",
    "    words_in_each_sentence = words_in_each_sentence.unsqueeze(0)\n",
    "\n",
    "    scores, word_alphas, sentence_alphas = model(doc, sentences_in_doc, words_in_each_sentence)\n",
    "\n",
    "    score, prediction_index = scores.max(dim=1)\n",
    "    score = float(score.exp()/scores.exp().sum())\n",
    "    prediction = rev_label_map[prediction_index.item()]\n",
    "    sen_len_norm = (words_in_each_sentence.unsqueeze(1).float() / words_in_each_sentence.max().float()).squeeze(0).squeeze(0)\n",
    "    sentence_alphas = sentence_alphas * sen_len_norm\n",
    "    alphas = torch.bmm(sentence_alphas.squeeze(0).unsqueeze(1).unsqueeze(1) , word_alphas.squeeze(0).unsqueeze(1)).squeeze(1)\n",
    "    alphas = alphas.to('cpu')\n",
    "\n",
    "    document = {\n",
    "        'classification': prediction,\n",
    "        'score': round(score, 2),\n",
    "        'lines': []\n",
    "    }\n",
    "\n",
    "    for s, sentence in enumerate(doc.squeeze(0)):\n",
    "        sentence_factor = sentence_alphas.squeeze(0)[s].item() / sentence_alphas.squeeze(0).max().item()\n",
    "        sentence_decoded = pre.word_tokenizer.convert_ids_to_tokens(sentence, skip_special_tokens=True)    \n",
    "        line = {\n",
    "            'score': sentence_factor,\n",
    "            'tokens': []\n",
    "        }\n",
    "        for w, word in enumerate(sentence_decoded):\n",
    "            word_factor = alphas[s, w].item() / alphas.max().item()\n",
    "            line['tokens'].append({'text': word.replace(\"Ä \", \"\"), 'score': word_factor})\n",
    "        document['lines'].append(line)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-18T05:22:52.289421Z",
     "start_time": "2021-01-18T05:22:52.036759Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1)\n",
    "\n",
    "for _ in range(10):\n",
    "    x = random.choice(X_test)\n",
    "    print(x)\n",
    "    output_doc(inference(model, pre, rev_label_map, x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
